{"timestamp": "2025-06-13 22:07:35", "os": "Windows-10-10.0.22631-SP0", "cpu": "Intel64 Family 6 Model 154 Stepping 3, GenuineIntel", "cpu_count": 14, "cpu_count_logical": 20, "gpu": "NVIDIA GeForce RTX 3070 Ti Laptop GPU", "device": "cuda", "model": "SmallConvNet CNN (3x64x64 input)", "results": [{"batch_size": 1, "total_time_50_forward_passes": 0.016004562377929688, "avg_time_per_forward_pass": 0.00032009124755859374, "avg_time_per_image": 0.00032009124755859374}, {"batch_size": 8, "total_time_50_forward_passes": 0.031256914138793945, "avg_time_per_forward_pass": 0.0006251382827758789, "avg_time_per_image": 7.814228534698486e-05}, {"batch_size": 32, "total_time_50_forward_passes": 0.04732966423034668, "avg_time_per_forward_pass": 0.0009465932846069336, "avg_time_per_image": 2.9581040143966676e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.17546772956848145, "avg_time_per_forward_pass": 0.0035093545913696287, "avg_time_per_image": 2.7416832745075224e-05}], "compiled_results": [{"error": "torch.compile failed with: backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"}]}
{"timestamp": "2025-06-13 22:07:40", "os": "Windows-10-10.0.22631-SP0", "cpu": "Intel64 Family 6 Model 154 Stepping 3, GenuineIntel", "cpu_count": 14, "cpu_count_logical": 20, "gpu": "NVIDIA GeForce RTX 3070 Ti Laptop GPU", "device": "cuda", "model": "SmallConvNet CNN (3x64x64 input)", "results": [{"batch_size": 1, "total_time_50_forward_passes": 0.03041243553161621, "avg_time_per_forward_pass": 0.0006082487106323242, "avg_time_per_image": 0.0006082487106323242}, {"batch_size": 8, "total_time_50_forward_passes": 0.02000594139099121, "avg_time_per_forward_pass": 0.0004001188278198242, "avg_time_per_image": 5.001485347747803e-05}, {"batch_size": 32, "total_time_50_forward_passes": 0.05006885528564453, "avg_time_per_forward_pass": 0.0010013771057128906, "avg_time_per_image": 3.129303455352783e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.17235755920410156, "avg_time_per_forward_pass": 0.003447151184082031, "avg_time_per_image": 2.693086862564087e-05}], "compiled_results": [{"error": "torch.compile failed with: backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"}]}
{"timestamp": "2025-06-13 22:07:45", "os": "Windows-10-10.0.22631-SP0", "cpu": "Intel64 Family 6 Model 154 Stepping 3, GenuineIntel", "cpu_count": 14, "cpu_count_logical": 20, "gpu": "NVIDIA GeForce RTX 3070 Ti Laptop GPU", "device": "cuda", "model": "SmallConvNet CNN (3x64x64 input)", "results": [{"batch_size": 1, "total_time_50_forward_passes": 0.03029632568359375, "avg_time_per_forward_pass": 0.000605926513671875, "avg_time_per_image": 0.000605926513671875}, {"batch_size": 8, "total_time_50_forward_passes": 0.020621061325073242, "avg_time_per_forward_pass": 0.00041242122650146483, "avg_time_per_image": 5.1552653312683104e-05}, {"batch_size": 32, "total_time_50_forward_passes": 0.05094003677368164, "avg_time_per_forward_pass": 0.0010188007354736328, "avg_time_per_image": 3.1837522983551024e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.17199444770812988, "avg_time_per_forward_pass": 0.0034398889541625978, "avg_time_per_image": 2.6874132454395295e-05}], "compiled_results": [{"error": "torch.compile failed with: backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"}]}
{"timestamp": "2025-06-13 21:13:05", "os": "Linux-6.6.87.1-microsoft-standard-WSL2-x86_64-with-glibc2.36", "cpu": "", "cpu_count": 10, "cpu_count_logical": 20, "gpu": "NVIDIA GeForce RTX 3070 Ti Laptop GPU", "device": "cuda", "model": "SmallConvNet CNN (3x64x64 input)", "results": [{"batch_size": 1, "total_time_50_forward_passes": 0.04680347442626953, "avg_time_per_forward_pass": 0.0009360694885253906, "avg_time_per_image": 0.0009360694885253906}, {"batch_size": 8, "total_time_50_forward_passes": 0.04113364219665527, "avg_time_per_forward_pass": 0.0008226728439331055, "avg_time_per_image": 0.00010283410549163818}, {"batch_size": 32, "total_time_50_forward_passes": 0.05895733833312988, "avg_time_per_forward_pass": 0.0011791467666625976, "avg_time_per_image": 3.6848336458206175e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.15831422805786133, "avg_time_per_forward_pass": 0.0031662845611572267, "avg_time_per_image": 2.4736598134040834e-05}], "compiled_results": [{"batch_size": 1, "total_time_50_forward_passes": 1.917531967163086, "avg_time_per_forward_pass": 0.038350639343261717, "avg_time_per_image": 0.038350639343261717}, {"batch_size": 8, "total_time_50_forward_passes": 1.2334115505218506, "avg_time_per_forward_pass": 0.02466823101043701, "avg_time_per_image": 0.0030835288763046264}, {"batch_size": 32, "total_time_50_forward_passes": 0.029889345169067383, "avg_time_per_forward_pass": 0.0005977869033813477, "avg_time_per_image": 1.8680840730667116e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.10699057579040527, "avg_time_per_forward_pass": 0.0021398115158081056, "avg_time_per_image": 1.6717277467250825e-05}]}
{"timestamp": "2025-06-13 21:13:23", "os": "Linux-6.6.87.1-microsoft-standard-WSL2-x86_64-with-glibc2.36", "cpu": "", "cpu_count": 10, "cpu_count_logical": 20, "gpu": "NVIDIA GeForce RTX 3070 Ti Laptop GPU", "device": "cuda", "model": "SmallConvNet CNN (3x64x64 input)", "results": [{"batch_size": 1, "total_time_50_forward_passes": 0.029072284698486328, "avg_time_per_forward_pass": 0.0005814456939697266, "avg_time_per_image": 0.0005814456939697266}, {"batch_size": 8, "total_time_50_forward_passes": 0.029165029525756836, "avg_time_per_forward_pass": 0.0005833005905151367, "avg_time_per_image": 7.291257381439209e-05}, {"batch_size": 32, "total_time_50_forward_passes": 0.05605459213256836, "avg_time_per_forward_pass": 0.0011210918426513671, "avg_time_per_image": 3.503412008285522e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.15578150749206543, "avg_time_per_forward_pass": 0.0031156301498413085, "avg_time_per_image": 2.4340860545635223e-05}], "compiled_results": [{"batch_size": 1, "total_time_50_forward_passes": 1.0489702224731445, "avg_time_per_forward_pass": 0.02097940444946289, "avg_time_per_image": 0.02097940444946289}, {"batch_size": 8, "total_time_50_forward_passes": 0.7931454181671143, "avg_time_per_forward_pass": 0.015862908363342285, "avg_time_per_image": 0.0019828635454177857}, {"batch_size": 32, "total_time_50_forward_passes": 0.03782987594604492, "avg_time_per_forward_pass": 0.0007565975189208985, "avg_time_per_image": 2.3643672466278077e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.1240389347076416, "avg_time_per_forward_pass": 0.002480778694152832, "avg_time_per_image": 1.9381083548069e-05}]}
{"timestamp": "2025-06-13 21:13:45", "os": "Linux-6.6.87.1-microsoft-standard-WSL2-x86_64-with-glibc2.36", "cpu": "", "cpu_count": 10, "cpu_count_logical": 20, "gpu": "NVIDIA GeForce RTX 3070 Ti Laptop GPU", "device": "cuda", "model": "SmallConvNet CNN (3x64x64 input)", "results": [{"batch_size": 1, "total_time_50_forward_passes": 0.030034303665161133, "avg_time_per_forward_pass": 0.0006006860733032227, "avg_time_per_image": 0.0006006860733032227}, {"batch_size": 8, "total_time_50_forward_passes": 0.03193187713623047, "avg_time_per_forward_pass": 0.0006386375427246093, "avg_time_per_image": 7.982969284057617e-05}, {"batch_size": 32, "total_time_50_forward_passes": 0.05810284614562988, "avg_time_per_forward_pass": 0.0011620569229125978, "avg_time_per_image": 3.631427884101868e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.1611478328704834, "avg_time_per_forward_pass": 0.003222956657409668, "avg_time_per_image": 2.517934888601303e-05}], "compiled_results": [{"batch_size": 1, "total_time_50_forward_passes": 1.0527524948120117, "avg_time_per_forward_pass": 0.021055049896240234, "avg_time_per_image": 0.021055049896240234}, {"batch_size": 8, "total_time_50_forward_passes": 0.7828099727630615, "avg_time_per_forward_pass": 0.01565619945526123, "avg_time_per_image": 0.001957024931907654}, {"batch_size": 32, "total_time_50_forward_passes": 0.038457393646240234, "avg_time_per_forward_pass": 0.0007691478729248047, "avg_time_per_image": 2.4035871028900146e-05}, {"batch_size": 128, "total_time_50_forward_passes": 0.12844538688659668, "avg_time_per_forward_pass": 0.0025689077377319337, "avg_time_per_image": 2.0069591701030732e-05}]}
