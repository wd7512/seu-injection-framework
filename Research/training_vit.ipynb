{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df832452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Save current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Change to parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(cwd, '..'))\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "# Temporarily add parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "import framework\n",
    "sys.path.pop(0)\n",
    "\n",
    "# Return to original directory\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce042a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "weights = ViT_B_32_Weights.DEFAULT\n",
    "# Transform matching ImageNet-trained model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    weights.transforms()\n",
    "])\n",
    "\n",
    "\n",
    "# CIFAR-10 test data\n",
    "dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeaa39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Load pretrained model\n",
    "model = vit_b_32(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "model.heads = nn.Linear(in_features = 768, out_features = 100, bias = True)\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec48ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repositories\\seu-injection-framework\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1/782, Loss: 4.7726\n",
      "Epoch 1, Batch 2/782, Loss: 4.7711\n",
      "Epoch 1, Batch 3/782, Loss: 4.6754\n",
      "Epoch 1, Batch 4/782, Loss: 4.7402\n",
      "Epoch 1, Batch 5/782, Loss: 4.7793\n",
      "Epoch 1, Batch 6/782, Loss: 4.8659\n",
      "Epoch 1, Batch 7/782, Loss: 5.0522\n",
      "Epoch 1, Batch 8/782, Loss: 4.9814\n",
      "Epoch 1, Batch 9/782, Loss: 4.9233\n",
      "Epoch 1, Batch 10/782, Loss: 4.8116\n",
      "Epoch 1, Batch 11/782, Loss: 4.8039\n",
      "Epoch 1, Batch 12/782, Loss: 4.7997\n",
      "Epoch 1, Batch 13/782, Loss: 4.9280\n",
      "Epoch 1, Batch 14/782, Loss: 5.0117\n",
      "Epoch 1, Batch 15/782, Loss: 4.6939\n",
      "Epoch 1, Batch 16/782, Loss: 4.8659\n",
      "Epoch 1, Batch 17/782, Loss: 4.6234\n",
      "Epoch 1, Batch 18/782, Loss: 4.6221\n",
      "Epoch 1, Batch 19/782, Loss: 4.7454\n",
      "Epoch 1, Batch 20/782, Loss: 4.8109\n",
      "Epoch 1, Batch 21/782, Loss: 4.8739\n",
      "Epoch 1, Batch 22/782, Loss: 4.7691\n",
      "Epoch 1, Batch 23/782, Loss: 4.7725\n",
      "Epoch 1, Batch 24/782, Loss: 4.8162\n",
      "Epoch 1, Batch 25/782, Loss: 4.7752\n",
      "Epoch 1, Batch 26/782, Loss: 4.8463\n",
      "Epoch 1, Batch 27/782, Loss: 4.7657\n",
      "Epoch 1, Batch 28/782, Loss: 4.7617\n",
      "Epoch 1, Batch 29/782, Loss: 4.6901\n",
      "Epoch 1, Batch 30/782, Loss: 4.6473\n",
      "Epoch 1, Batch 31/782, Loss: 4.7683\n",
      "Epoch 1, Batch 32/782, Loss: 4.6613\n",
      "Epoch 1, Batch 33/782, Loss: 4.7199\n",
      "Epoch 1, Batch 34/782, Loss: 4.7922\n",
      "Epoch 1, Batch 35/782, Loss: 4.8166\n",
      "Epoch 1, Batch 36/782, Loss: 4.6368\n",
      "Epoch 1, Batch 37/782, Loss: 4.6455\n",
      "Epoch 1, Batch 38/782, Loss: 4.6810\n",
      "Epoch 1, Batch 39/782, Loss: 4.7769\n",
      "Epoch 1, Batch 40/782, Loss: 4.6354\n",
      "Epoch 1, Batch 41/782, Loss: 4.6847\n",
      "Epoch 1, Batch 42/782, Loss: 4.6612\n",
      "Epoch 1, Batch 43/782, Loss: 4.6441\n",
      "Epoch 1, Batch 44/782, Loss: 4.6337\n",
      "Epoch 1, Batch 45/782, Loss: 4.6175\n",
      "Epoch 1, Batch 46/782, Loss: 4.4793\n",
      "Epoch 1, Batch 47/782, Loss: 4.6124\n",
      "Epoch 1, Batch 48/782, Loss: 4.5464\n",
      "Epoch 1, Batch 49/782, Loss: 4.6363\n",
      "Epoch 1, Batch 50/782, Loss: 4.6117\n",
      "Epoch 1, Batch 51/782, Loss: 4.6475\n",
      "Epoch 1, Batch 52/782, Loss: 4.6924\n",
      "Epoch 1, Batch 53/782, Loss: 4.5283\n",
      "Epoch 1, Batch 54/782, Loss: 4.5589\n",
      "Epoch 1, Batch 55/782, Loss: 4.6334\n",
      "Epoch 1, Batch 56/782, Loss: 4.5683\n",
      "Epoch 1, Batch 57/782, Loss: 4.5840\n",
      "Epoch 1, Batch 58/782, Loss: 4.5695\n",
      "Epoch 1, Batch 59/782, Loss: 4.5780\n",
      "Epoch 1, Batch 60/782, Loss: 4.6586\n",
      "Epoch 1, Batch 61/782, Loss: 4.4767\n",
      "Epoch 1, Batch 62/782, Loss: 4.5588\n",
      "Epoch 1, Batch 63/782, Loss: 4.6314\n",
      "Epoch 1, Batch 64/782, Loss: 4.4791\n",
      "Epoch 1, Batch 65/782, Loss: 4.5564\n",
      "Epoch 1, Batch 66/782, Loss: 4.4595\n",
      "Epoch 1, Batch 67/782, Loss: 4.4714\n",
      "Epoch 1, Batch 68/782, Loss: 4.4349\n",
      "Epoch 1, Batch 69/782, Loss: 4.5769\n",
      "Epoch 1, Batch 70/782, Loss: 4.5116\n",
      "Epoch 1, Batch 71/782, Loss: 4.4392\n",
      "Epoch 1, Batch 72/782, Loss: 4.4086\n",
      "Epoch 1, Batch 73/782, Loss: 4.4453\n",
      "Epoch 1, Batch 74/782, Loss: 4.4614\n",
      "Epoch 1, Batch 75/782, Loss: 4.3726\n",
      "Epoch 1, Batch 76/782, Loss: 4.3327\n",
      "Epoch 1, Batch 77/782, Loss: 4.4274\n",
      "Epoch 1, Batch 78/782, Loss: 4.4692\n",
      "Epoch 1, Batch 79/782, Loss: 4.4314\n",
      "Epoch 1, Batch 80/782, Loss: 4.4703\n",
      "Epoch 1, Batch 81/782, Loss: 4.3520\n",
      "Epoch 1, Batch 82/782, Loss: 4.3745\n",
      "Epoch 1, Batch 83/782, Loss: 4.4446\n",
      "Epoch 1, Batch 84/782, Loss: 4.4776\n",
      "Epoch 1, Batch 85/782, Loss: 4.4476\n",
      "Epoch 1, Batch 86/782, Loss: 4.3677\n",
      "Epoch 1, Batch 87/782, Loss: 4.2704\n",
      "Epoch 1, Batch 88/782, Loss: 4.2852\n",
      "Epoch 1, Batch 89/782, Loss: 4.4934\n",
      "Epoch 1, Batch 90/782, Loss: 4.5391\n",
      "Epoch 1, Batch 91/782, Loss: 4.4037\n",
      "Epoch 1, Batch 92/782, Loss: 4.4590\n",
      "Epoch 1, Batch 93/782, Loss: 4.4281\n",
      "Epoch 1, Batch 94/782, Loss: 4.4908\n",
      "Epoch 1, Batch 95/782, Loss: 4.4357\n",
      "Epoch 1, Batch 96/782, Loss: 4.4458\n",
      "Epoch 1, Batch 97/782, Loss: 4.4791\n",
      "Epoch 1, Batch 98/782, Loss: 4.2985\n",
      "Epoch 1, Batch 99/782, Loss: 4.3566\n",
      "Epoch 1, Batch 100/782, Loss: 4.3759\n",
      "Epoch 1, Batch 101/782, Loss: 4.1926\n",
      "Epoch 1, Batch 102/782, Loss: 4.3467\n",
      "Epoch 1, Batch 103/782, Loss: 4.2944\n",
      "Epoch 1, Batch 104/782, Loss: 4.3457\n",
      "Epoch 1, Batch 105/782, Loss: 4.3724\n",
      "Epoch 1, Batch 106/782, Loss: 4.2377\n",
      "Epoch 1, Batch 107/782, Loss: 4.1633\n",
      "Epoch 1, Batch 108/782, Loss: 4.2040\n",
      "Epoch 1, Batch 109/782, Loss: 4.3706\n",
      "Epoch 1, Batch 110/782, Loss: 4.1564\n",
      "Epoch 1, Batch 111/782, Loss: 4.2957\n",
      "Epoch 1, Batch 112/782, Loss: 4.3319\n",
      "Epoch 1, Batch 113/782, Loss: 4.2103\n",
      "Epoch 1, Batch 114/782, Loss: 4.4815\n",
      "Epoch 1, Batch 115/782, Loss: 4.2648\n",
      "Epoch 1, Batch 116/782, Loss: 4.2608\n",
      "Epoch 1, Batch 117/782, Loss: 4.4182\n",
      "Epoch 1, Batch 118/782, Loss: 4.2633\n",
      "Epoch 1, Batch 119/782, Loss: 4.2220\n",
      "Epoch 1, Batch 120/782, Loss: 4.3679\n",
      "Epoch 1, Batch 121/782, Loss: 4.3279\n",
      "Epoch 1, Batch 122/782, Loss: 4.1545\n",
      "Epoch 1, Batch 123/782, Loss: 4.1331\n",
      "Epoch 1, Batch 124/782, Loss: 4.2005\n",
      "Epoch 1, Batch 125/782, Loss: 4.2743\n",
      "Epoch 1, Batch 126/782, Loss: 4.3025\n",
      "Epoch 1, Batch 127/782, Loss: 4.0378\n",
      "Epoch 1, Batch 128/782, Loss: 4.1715\n",
      "Epoch 1, Batch 129/782, Loss: 4.1996\n",
      "Epoch 1, Batch 130/782, Loss: 4.1440\n",
      "Epoch 1, Batch 131/782, Loss: 4.3935\n",
      "Epoch 1, Batch 132/782, Loss: 4.3604\n",
      "Epoch 1, Batch 133/782, Loss: 4.1927\n",
      "Epoch 1, Batch 134/782, Loss: 4.3056\n",
      "Epoch 1, Batch 135/782, Loss: 4.2867\n",
      "Epoch 1, Batch 136/782, Loss: 4.4154\n",
      "Epoch 1, Batch 137/782, Loss: 4.4316\n",
      "Epoch 1, Batch 138/782, Loss: 4.2343\n",
      "Epoch 1, Batch 139/782, Loss: 4.1269\n",
      "Epoch 1, Batch 140/782, Loss: 4.3557\n",
      "Epoch 1, Batch 141/782, Loss: 4.0236\n",
      "Epoch 1, Batch 142/782, Loss: 4.1399\n",
      "Epoch 1, Batch 143/782, Loss: 4.3521\n",
      "Epoch 1, Batch 144/782, Loss: 4.3311\n",
      "Epoch 1, Batch 145/782, Loss: 4.1873\n",
      "Epoch 1, Batch 146/782, Loss: 4.1882\n",
      "Epoch 1, Batch 147/782, Loss: 4.1418\n",
      "Epoch 1, Batch 148/782, Loss: 4.3596\n",
      "Epoch 1, Batch 149/782, Loss: 4.0760\n",
      "Epoch 1, Batch 150/782, Loss: 4.2939\n",
      "Epoch 1, Batch 151/782, Loss: 4.1397\n",
      "Epoch 1, Batch 152/782, Loss: 4.2980\n",
      "Epoch 1, Batch 153/782, Loss: 4.1918\n",
      "Epoch 1, Batch 154/782, Loss: 4.1832\n",
      "Epoch 1, Batch 155/782, Loss: 4.1266\n",
      "Epoch 1, Batch 156/782, Loss: 4.1433\n",
      "Epoch 1, Batch 157/782, Loss: 4.1185\n",
      "Epoch 1, Batch 158/782, Loss: 4.2774\n",
      "Epoch 1, Batch 159/782, Loss: 4.2371\n",
      "Epoch 1, Batch 160/782, Loss: 4.3798\n",
      "Epoch 1, Batch 161/782, Loss: 4.2051\n",
      "Epoch 1, Batch 162/782, Loss: 4.0612\n",
      "Epoch 1, Batch 163/782, Loss: 4.1488\n",
      "Epoch 1, Batch 164/782, Loss: 4.0514\n",
      "Epoch 1, Batch 165/782, Loss: 4.4695\n",
      "Epoch 1, Batch 166/782, Loss: 3.8551\n",
      "Epoch 1, Batch 167/782, Loss: 4.1227\n",
      "Epoch 1, Batch 168/782, Loss: 4.2084\n",
      "Epoch 1, Batch 169/782, Loss: 4.0063\n",
      "Epoch 1, Batch 170/782, Loss: 4.2015\n",
      "Epoch 1, Batch 171/782, Loss: 4.1892\n",
      "Epoch 1, Batch 172/782, Loss: 4.3057\n",
      "Epoch 1, Batch 173/782, Loss: 4.3052\n",
      "Epoch 1, Batch 174/782, Loss: 4.1672\n",
      "Epoch 1, Batch 175/782, Loss: 4.1796\n",
      "Epoch 1, Batch 176/782, Loss: 4.3477\n",
      "Epoch 1, Batch 177/782, Loss: 4.2674\n",
      "Epoch 1, Batch 178/782, Loss: 4.0593\n",
      "Epoch 1, Batch 179/782, Loss: 3.9973\n",
      "Epoch 1, Batch 180/782, Loss: 4.1820\n",
      "Epoch 1, Batch 181/782, Loss: 4.1844\n",
      "Epoch 1, Batch 182/782, Loss: 4.1389\n",
      "Epoch 1, Batch 183/782, Loss: 4.0277\n",
      "Epoch 1, Batch 184/782, Loss: 4.2280\n",
      "Epoch 1, Batch 185/782, Loss: 3.9882\n",
      "Epoch 1, Batch 186/782, Loss: 4.0287\n",
      "Epoch 1, Batch 187/782, Loss: 4.1226\n",
      "Epoch 1, Batch 188/782, Loss: 4.0485\n",
      "Epoch 1, Batch 189/782, Loss: 4.1390\n",
      "Epoch 1, Batch 190/782, Loss: 4.1328\n",
      "Epoch 1, Batch 191/782, Loss: 3.8945\n",
      "Epoch 1, Batch 192/782, Loss: 4.0465\n",
      "Epoch 1, Batch 193/782, Loss: 4.0250\n",
      "Epoch 1, Batch 194/782, Loss: 4.0413\n",
      "Epoch 1, Batch 195/782, Loss: 3.9079\n",
      "Epoch 1, Batch 196/782, Loss: 4.2141\n",
      "Epoch 1, Batch 197/782, Loss: 4.0291\n",
      "Epoch 1, Batch 198/782, Loss: 4.1698\n",
      "Epoch 1, Batch 199/782, Loss: 3.8909\n",
      "Epoch 1, Batch 200/782, Loss: 4.1721\n",
      "Epoch 1, Batch 201/782, Loss: 4.0151\n",
      "Epoch 1, Batch 202/782, Loss: 3.9680\n",
      "Epoch 1, Batch 203/782, Loss: 4.0702\n",
      "Epoch 1, Batch 204/782, Loss: 4.0987\n",
      "Epoch 1, Batch 205/782, Loss: 3.9950\n",
      "Epoch 1, Batch 206/782, Loss: 3.9401\n",
      "Epoch 1, Batch 207/782, Loss: 4.0626\n",
      "Epoch 1, Batch 208/782, Loss: 4.1540\n",
      "Epoch 1, Batch 209/782, Loss: 4.1225\n",
      "Epoch 1, Batch 210/782, Loss: 4.1861\n",
      "Epoch 1, Batch 211/782, Loss: 4.0513\n",
      "Epoch 1, Batch 212/782, Loss: 4.1716\n",
      "Epoch 1, Batch 213/782, Loss: 3.9088\n",
      "Epoch 1, Batch 214/782, Loss: 3.7848\n",
      "Epoch 1, Batch 215/782, Loss: 4.1366\n",
      "Epoch 1, Batch 216/782, Loss: 3.9134\n",
      "Epoch 1, Batch 217/782, Loss: 3.9039\n",
      "Epoch 1, Batch 218/782, Loss: 3.8827\n",
      "Epoch 1, Batch 219/782, Loss: 3.9760\n",
      "Epoch 1, Batch 220/782, Loss: 3.9088\n",
      "Epoch 1, Batch 221/782, Loss: 3.8217\n",
      "Epoch 1, Batch 222/782, Loss: 4.1562\n",
      "Epoch 1, Batch 223/782, Loss: 3.7880\n",
      "Epoch 1, Batch 224/782, Loss: 3.8565\n",
      "Epoch 1, Batch 225/782, Loss: 4.0591\n",
      "Epoch 1, Batch 226/782, Loss: 3.8675\n",
      "Epoch 1, Batch 227/782, Loss: 3.8470\n",
      "Epoch 1, Batch 228/782, Loss: 4.2902\n",
      "Epoch 1, Batch 229/782, Loss: 4.0360\n",
      "Epoch 1, Batch 230/782, Loss: 3.6543\n",
      "Epoch 1, Batch 231/782, Loss: 4.1650\n",
      "Epoch 1, Batch 232/782, Loss: 4.0609\n",
      "Epoch 1, Batch 233/782, Loss: 3.8398\n",
      "Epoch 1, Batch 234/782, Loss: 3.9774\n",
      "Epoch 1, Batch 235/782, Loss: 4.1737\n",
      "Epoch 1, Batch 236/782, Loss: 4.0415\n",
      "Epoch 1, Batch 237/782, Loss: 4.1926\n",
      "Epoch 1, Batch 238/782, Loss: 4.0438\n",
      "Epoch 1, Batch 239/782, Loss: 3.8929\n",
      "Epoch 1, Batch 240/782, Loss: 4.0956\n",
      "Epoch 1, Batch 241/782, Loss: 4.1667\n",
      "Epoch 1, Batch 242/782, Loss: 3.9855\n",
      "Epoch 1, Batch 243/782, Loss: 4.0094\n",
      "Epoch 1, Batch 244/782, Loss: 3.8451\n",
      "Epoch 1, Batch 245/782, Loss: 4.1915\n",
      "Epoch 1, Batch 246/782, Loss: 4.1587\n",
      "Epoch 1, Batch 247/782, Loss: 3.9765\n",
      "Epoch 1, Batch 248/782, Loss: 3.7915\n",
      "Epoch 1, Batch 249/782, Loss: 3.8882\n",
      "Epoch 1, Batch 250/782, Loss: 3.8645\n",
      "Epoch 1, Batch 251/782, Loss: 3.9461\n",
      "Epoch 1, Batch 252/782, Loss: 4.1441\n",
      "Epoch 1, Batch 253/782, Loss: 4.0885\n",
      "Epoch 1, Batch 254/782, Loss: 3.7962\n",
      "Epoch 1, Batch 255/782, Loss: 4.2377\n",
      "Epoch 1, Batch 256/782, Loss: 4.2647\n",
      "Epoch 1, Batch 257/782, Loss: 4.0702\n",
      "Epoch 1, Batch 258/782, Loss: 3.8820\n",
      "Epoch 1, Batch 259/782, Loss: 4.0149\n",
      "Epoch 1, Batch 260/782, Loss: 3.8101\n",
      "Epoch 1, Batch 261/782, Loss: 3.8825\n",
      "Epoch 1, Batch 262/782, Loss: 3.9174\n",
      "Epoch 1, Batch 263/782, Loss: 3.9120\n",
      "Epoch 1, Batch 264/782, Loss: 4.0079\n",
      "Epoch 1, Batch 265/782, Loss: 4.2045\n",
      "Epoch 1, Batch 266/782, Loss: 3.9739\n",
      "Epoch 1, Batch 267/782, Loss: 3.5844\n",
      "Epoch 1, Batch 268/782, Loss: 3.9985\n",
      "Epoch 1, Batch 269/782, Loss: 3.7763\n",
      "Epoch 1, Batch 270/782, Loss: 4.0493\n",
      "Epoch 1, Batch 271/782, Loss: 3.9474\n",
      "Epoch 1, Batch 272/782, Loss: 3.9153\n",
      "Epoch 1, Batch 273/782, Loss: 4.1884\n",
      "Epoch 1, Batch 274/782, Loss: 3.9810\n",
      "Epoch 1, Batch 275/782, Loss: 3.9919\n",
      "Epoch 1, Batch 276/782, Loss: 3.8482\n",
      "Epoch 1, Batch 277/782, Loss: 4.0333\n",
      "Epoch 1, Batch 278/782, Loss: 3.9430\n",
      "Epoch 1, Batch 279/782, Loss: 3.7230\n",
      "Epoch 1, Batch 280/782, Loss: 3.6316\n",
      "Epoch 1, Batch 281/782, Loss: 3.4967\n",
      "Epoch 1, Batch 282/782, Loss: 3.7246\n",
      "Epoch 1, Batch 283/782, Loss: 3.7562\n",
      "Epoch 1, Batch 284/782, Loss: 3.8162\n",
      "Epoch 1, Batch 285/782, Loss: 4.0228\n",
      "Epoch 1, Batch 286/782, Loss: 3.6820\n",
      "Epoch 1, Batch 287/782, Loss: 3.9238\n",
      "Epoch 1, Batch 288/782, Loss: 3.8646\n",
      "Epoch 1, Batch 289/782, Loss: 3.8478\n",
      "Epoch 1, Batch 290/782, Loss: 3.8479\n",
      "Epoch 1, Batch 291/782, Loss: 3.8861\n",
      "Epoch 1, Batch 292/782, Loss: 3.8210\n",
      "Epoch 1, Batch 293/782, Loss: 4.0190\n",
      "Epoch 1, Batch 294/782, Loss: 4.2971\n",
      "Epoch 1, Batch 295/782, Loss: 3.9024\n",
      "Epoch 1, Batch 296/782, Loss: 4.0595\n",
      "Epoch 1, Batch 297/782, Loss: 3.9844\n",
      "Epoch 1, Batch 298/782, Loss: 3.9576\n",
      "Epoch 1, Batch 299/782, Loss: 4.0346\n",
      "Epoch 1, Batch 300/782, Loss: 3.8200\n",
      "Epoch 1, Batch 301/782, Loss: 3.8820\n",
      "Epoch 1, Batch 302/782, Loss: 4.1025\n",
      "Epoch 1, Batch 303/782, Loss: 3.9540\n",
      "Epoch 1, Batch 304/782, Loss: 3.7391\n",
      "Epoch 1, Batch 305/782, Loss: 3.9534\n",
      "Epoch 1, Batch 306/782, Loss: 4.0153\n",
      "Epoch 1, Batch 307/782, Loss: 3.7712\n",
      "Epoch 1, Batch 308/782, Loss: 3.7216\n",
      "Epoch 1, Batch 309/782, Loss: 3.9935\n",
      "Epoch 1, Batch 310/782, Loss: 4.0568\n",
      "Epoch 1, Batch 311/782, Loss: 3.8470\n",
      "Epoch 1, Batch 312/782, Loss: 3.9628\n",
      "Epoch 1, Batch 313/782, Loss: 3.9276\n",
      "Epoch 1, Batch 314/782, Loss: 3.7802\n",
      "Epoch 1, Batch 315/782, Loss: 3.7534\n",
      "Epoch 1, Batch 316/782, Loss: 4.0807\n",
      "Epoch 1, Batch 317/782, Loss: 3.8403\n",
      "Epoch 1, Batch 318/782, Loss: 3.6167\n",
      "Epoch 1, Batch 319/782, Loss: 3.8603\n",
      "Epoch 1, Batch 320/782, Loss: 3.7290\n",
      "Epoch 1, Batch 321/782, Loss: 3.7462\n",
      "Epoch 1, Batch 322/782, Loss: 3.8391\n",
      "Epoch 1, Batch 323/782, Loss: 3.8628\n",
      "Epoch 1, Batch 324/782, Loss: 3.9742\n",
      "Epoch 1, Batch 325/782, Loss: 3.7054\n",
      "Epoch 1, Batch 326/782, Loss: 3.9608\n",
      "Epoch 1, Batch 327/782, Loss: 4.0456\n",
      "Epoch 1, Batch 328/782, Loss: 3.7613\n",
      "Epoch 1, Batch 329/782, Loss: 4.0311\n",
      "Epoch 1, Batch 330/782, Loss: 3.9149\n",
      "Epoch 1, Batch 331/782, Loss: 3.7235\n",
      "Epoch 1, Batch 332/782, Loss: 3.8351\n",
      "Epoch 1, Batch 333/782, Loss: 4.0047\n",
      "Epoch 1, Batch 334/782, Loss: 3.8106\n",
      "Epoch 1, Batch 335/782, Loss: 4.1278\n",
      "Epoch 1, Batch 336/782, Loss: 3.8811\n",
      "Epoch 1, Batch 337/782, Loss: 3.7552\n",
      "Epoch 1, Batch 338/782, Loss: 3.8970\n",
      "Epoch 1, Batch 339/782, Loss: 4.0893\n",
      "Epoch 1, Batch 340/782, Loss: 3.7979\n",
      "Epoch 1, Batch 341/782, Loss: 3.9550\n",
      "Epoch 1, Batch 342/782, Loss: 3.9197\n",
      "Epoch 1, Batch 343/782, Loss: 3.7049\n",
      "Epoch 1, Batch 344/782, Loss: 3.8670\n",
      "Epoch 1, Batch 345/782, Loss: 3.9239\n",
      "Epoch 1, Batch 346/782, Loss: 3.6456\n",
      "Epoch 1, Batch 347/782, Loss: 4.1261\n",
      "Epoch 1, Batch 348/782, Loss: 3.8862\n",
      "Epoch 1, Batch 349/782, Loss: 3.9730\n",
      "Epoch 1, Batch 350/782, Loss: 3.6274\n",
      "Epoch 1, Batch 351/782, Loss: 3.8466\n",
      "Epoch 1, Batch 352/782, Loss: 3.7184\n",
      "Epoch 1, Batch 353/782, Loss: 3.8073\n",
      "Epoch 1, Batch 354/782, Loss: 4.1258\n",
      "Epoch 1, Batch 355/782, Loss: 3.7106\n",
      "Epoch 1, Batch 356/782, Loss: 3.8058\n",
      "Epoch 1, Batch 357/782, Loss: 3.5841\n",
      "Epoch 1, Batch 358/782, Loss: 3.7041\n",
      "Epoch 1, Batch 359/782, Loss: 3.7896\n",
      "Epoch 1, Batch 360/782, Loss: 3.5910\n",
      "Epoch 1, Batch 361/782, Loss: 3.8112\n",
      "Epoch 1, Batch 362/782, Loss: 3.7434\n",
      "Epoch 1, Batch 363/782, Loss: 3.9084\n",
      "Epoch 1, Batch 364/782, Loss: 3.9343\n",
      "Epoch 1, Batch 365/782, Loss: 3.8871\n",
      "Epoch 1, Batch 366/782, Loss: 3.6652\n",
      "Epoch 1, Batch 367/782, Loss: 3.8688\n",
      "Epoch 1, Batch 368/782, Loss: 3.7162\n",
      "Epoch 1, Batch 369/782, Loss: 3.7119\n",
      "Epoch 1, Batch 370/782, Loss: 3.9322\n",
      "Epoch 1, Batch 371/782, Loss: 3.7624\n",
      "Epoch 1, Batch 372/782, Loss: 3.9570\n",
      "Epoch 1, Batch 373/782, Loss: 3.9159\n",
      "Epoch 1, Batch 374/782, Loss: 3.6310\n",
      "Epoch 1, Batch 375/782, Loss: 3.8340\n",
      "Epoch 1, Batch 376/782, Loss: 3.5161\n",
      "Epoch 1, Batch 377/782, Loss: 3.7731\n",
      "Epoch 1, Batch 378/782, Loss: 3.6966\n",
      "Epoch 1, Batch 379/782, Loss: 3.8004\n",
      "Epoch 1, Batch 380/782, Loss: 3.6700\n",
      "Epoch 1, Batch 381/782, Loss: 3.6455\n",
      "Epoch 1, Batch 382/782, Loss: 3.7259\n",
      "Epoch 1, Batch 383/782, Loss: 3.9052\n",
      "Epoch 1, Batch 384/782, Loss: 3.6343\n",
      "Epoch 1, Batch 385/782, Loss: 3.3844\n",
      "Epoch 1, Batch 386/782, Loss: 3.4476\n",
      "Epoch 1, Batch 387/782, Loss: 3.8774\n",
      "Epoch 1, Batch 388/782, Loss: 3.8546\n",
      "Epoch 1, Batch 389/782, Loss: 3.6159\n",
      "Epoch 1, Batch 390/782, Loss: 3.7394\n",
      "Epoch 1, Batch 391/782, Loss: 3.9235\n",
      "Epoch 1, Batch 392/782, Loss: 3.6732\n",
      "Epoch 1, Batch 393/782, Loss: 4.0644\n",
      "Epoch 1, Batch 394/782, Loss: 4.0632\n",
      "Epoch 1, Batch 395/782, Loss: 3.6469\n",
      "Epoch 1, Batch 396/782, Loss: 3.9237\n",
      "Epoch 1, Batch 397/782, Loss: 3.9771\n",
      "Epoch 1, Batch 398/782, Loss: 3.6094\n",
      "Epoch 1, Batch 399/782, Loss: 3.7158\n",
      "Epoch 1, Batch 400/782, Loss: 3.9216\n",
      "Epoch 1, Batch 401/782, Loss: 3.7193\n",
      "Epoch 1, Batch 402/782, Loss: 3.5661\n",
      "Epoch 1, Batch 403/782, Loss: 3.8690\n",
      "Epoch 1, Batch 404/782, Loss: 3.7181\n",
      "Epoch 1, Batch 405/782, Loss: 4.0215\n",
      "Epoch 1, Batch 406/782, Loss: 3.6907\n",
      "Epoch 1, Batch 407/782, Loss: 4.0106\n",
      "Epoch 1, Batch 408/782, Loss: 4.2764\n",
      "Epoch 1, Batch 409/782, Loss: 3.6851\n",
      "Epoch 1, Batch 410/782, Loss: 3.7493\n",
      "Epoch 1, Batch 411/782, Loss: 3.8376\n",
      "Epoch 1, Batch 412/782, Loss: 3.5321\n",
      "Epoch 1, Batch 413/782, Loss: 3.6739\n",
      "Epoch 1, Batch 414/782, Loss: 3.6231\n",
      "Epoch 1, Batch 415/782, Loss: 3.7389\n",
      "Epoch 1, Batch 416/782, Loss: 3.8514\n",
      "Epoch 1, Batch 417/782, Loss: 3.9457\n",
      "Epoch 1, Batch 418/782, Loss: 3.6769\n",
      "Epoch 1, Batch 419/782, Loss: 3.7494\n",
      "Epoch 1, Batch 420/782, Loss: 3.7744\n",
      "Epoch 1, Batch 421/782, Loss: 3.7639\n",
      "Epoch 1, Batch 422/782, Loss: 3.7047\n",
      "Epoch 1, Batch 423/782, Loss: 3.8021\n",
      "Epoch 1, Batch 424/782, Loss: 3.6706\n",
      "Epoch 1, Batch 425/782, Loss: 3.6647\n",
      "Epoch 1, Batch 426/782, Loss: 3.5753\n",
      "Epoch 1, Batch 427/782, Loss: 3.7958\n",
      "Epoch 1, Batch 428/782, Loss: 3.5216\n",
      "Epoch 1, Batch 429/782, Loss: 3.7382\n",
      "Epoch 1, Batch 430/782, Loss: 3.6215\n",
      "Epoch 1, Batch 431/782, Loss: 3.6899\n",
      "Epoch 1, Batch 432/782, Loss: 3.7556\n",
      "Epoch 1, Batch 433/782, Loss: 3.5907\n",
      "Epoch 1, Batch 434/782, Loss: 3.6494\n",
      "Epoch 1, Batch 435/782, Loss: 3.3538\n",
      "Epoch 1, Batch 436/782, Loss: 3.6746\n",
      "Epoch 1, Batch 437/782, Loss: 3.9008\n",
      "Epoch 1, Batch 438/782, Loss: 3.5590\n",
      "Epoch 1, Batch 439/782, Loss: 3.7026\n",
      "Epoch 1, Batch 440/782, Loss: 3.6857\n",
      "Epoch 1, Batch 441/782, Loss: 3.4683\n",
      "Epoch 1, Batch 442/782, Loss: 3.7549\n",
      "Epoch 1, Batch 443/782, Loss: 3.6776\n",
      "Epoch 1, Batch 444/782, Loss: 3.8421\n",
      "Epoch 1, Batch 445/782, Loss: 3.4739\n",
      "Epoch 1, Batch 446/782, Loss: 3.6163\n",
      "Epoch 1, Batch 447/782, Loss: 3.7121\n",
      "Epoch 1, Batch 448/782, Loss: 3.5442\n",
      "Epoch 1, Batch 449/782, Loss: 3.6720\n",
      "Epoch 1, Batch 450/782, Loss: 4.0055\n",
      "Epoch 1, Batch 451/782, Loss: 3.7416\n",
      "Epoch 1, Batch 452/782, Loss: 3.8046\n",
      "Epoch 1, Batch 453/782, Loss: 3.6670\n",
      "Epoch 1, Batch 454/782, Loss: 3.7155\n",
      "Epoch 1, Batch 455/782, Loss: 3.5190\n",
      "Epoch 1, Batch 456/782, Loss: 3.7850\n",
      "Epoch 1, Batch 457/782, Loss: 3.7555\n",
      "Epoch 1, Batch 458/782, Loss: 4.0524\n",
      "Epoch 1, Batch 459/782, Loss: 3.3697\n",
      "Epoch 1, Batch 460/782, Loss: 3.8635\n",
      "Epoch 1, Batch 461/782, Loss: 3.9157\n",
      "Epoch 1, Batch 462/782, Loss: 3.6942\n",
      "Epoch 1, Batch 463/782, Loss: 3.9139\n",
      "Epoch 1, Batch 464/782, Loss: 3.9193\n",
      "Epoch 1, Batch 465/782, Loss: 3.7654\n",
      "Epoch 1, Batch 466/782, Loss: 3.6106\n",
      "Epoch 1, Batch 467/782, Loss: 3.5649\n",
      "Epoch 1, Batch 468/782, Loss: 3.7537\n",
      "Epoch 1, Batch 469/782, Loss: 3.6268\n",
      "Epoch 1, Batch 470/782, Loss: 3.7317\n",
      "Epoch 1, Batch 471/782, Loss: 3.8044\n",
      "Epoch 1, Batch 472/782, Loss: 3.8656\n",
      "Epoch 1, Batch 473/782, Loss: 3.7021\n",
      "Epoch 1, Batch 474/782, Loss: 3.8451\n",
      "Epoch 1, Batch 475/782, Loss: 3.5933\n",
      "Epoch 1, Batch 476/782, Loss: 3.7967\n",
      "Epoch 1, Batch 477/782, Loss: 3.5621\n",
      "Epoch 1, Batch 478/782, Loss: 3.7535\n",
      "Epoch 1, Batch 479/782, Loss: 3.9567\n",
      "Epoch 1, Batch 480/782, Loss: 3.8443\n",
      "Epoch 1, Batch 481/782, Loss: 3.6019\n",
      "Epoch 1, Batch 482/782, Loss: 3.8646\n",
      "Epoch 1, Batch 483/782, Loss: 3.5282\n",
      "Epoch 1, Batch 484/782, Loss: 3.6822\n",
      "Epoch 1, Batch 485/782, Loss: 3.3702\n",
      "Epoch 1, Batch 486/782, Loss: 3.4795\n",
      "Epoch 1, Batch 487/782, Loss: 3.4837\n",
      "Epoch 1, Batch 488/782, Loss: 3.4713\n",
      "Epoch 1, Batch 489/782, Loss: 3.6080\n",
      "Epoch 1, Batch 490/782, Loss: 3.6067\n",
      "Epoch 1, Batch 491/782, Loss: 3.9849\n",
      "Epoch 1, Batch 492/782, Loss: 3.6201\n",
      "Epoch 1, Batch 493/782, Loss: 3.7384\n",
      "Epoch 1, Batch 494/782, Loss: 3.7320\n",
      "Epoch 1, Batch 495/782, Loss: 3.6119\n",
      "Epoch 1, Batch 496/782, Loss: 3.8398\n",
      "Epoch 1, Batch 497/782, Loss: 3.7756\n",
      "Epoch 1, Batch 498/782, Loss: 4.1989\n",
      "Epoch 1, Batch 499/782, Loss: 4.0090\n",
      "Epoch 1, Batch 500/782, Loss: 3.6767\n",
      "Epoch 1, Batch 501/782, Loss: 3.5448\n",
      "Epoch 1, Batch 502/782, Loss: 3.6982\n",
      "Epoch 1, Batch 503/782, Loss: 3.8165\n",
      "Epoch 1, Batch 504/782, Loss: 3.6946\n",
      "Epoch 1, Batch 505/782, Loss: 3.7571\n",
      "Epoch 1, Batch 506/782, Loss: 3.9042\n",
      "Epoch 1, Batch 507/782, Loss: 3.6777\n",
      "Epoch 1, Batch 508/782, Loss: 3.6579\n",
      "Epoch 1, Batch 509/782, Loss: 3.6326\n",
      "Epoch 1, Batch 510/782, Loss: 3.4467\n",
      "Epoch 1, Batch 511/782, Loss: 3.6067\n",
      "Epoch 1, Batch 512/782, Loss: 3.4673\n",
      "Epoch 1, Batch 513/782, Loss: 3.7075\n",
      "Epoch 1, Batch 514/782, Loss: 3.7886\n",
      "Epoch 1, Batch 515/782, Loss: 3.7450\n",
      "Epoch 1, Batch 516/782, Loss: 3.6602\n",
      "Epoch 1, Batch 517/782, Loss: 3.9582\n",
      "Epoch 1, Batch 518/782, Loss: 3.9487\n",
      "Epoch 1, Batch 519/782, Loss: 3.7462\n",
      "Epoch 1, Batch 520/782, Loss: 3.7903\n",
      "Epoch 1, Batch 521/782, Loss: 3.9631\n",
      "Epoch 1, Batch 522/782, Loss: 3.9304\n",
      "Epoch 1, Batch 523/782, Loss: 3.7069\n",
      "Epoch 1, Batch 524/782, Loss: 3.6973\n",
      "Epoch 1, Batch 525/782, Loss: 3.7291\n",
      "Epoch 1, Batch 526/782, Loss: 3.7589\n",
      "Epoch 1, Batch 527/782, Loss: 3.5351\n",
      "Epoch 1, Batch 528/782, Loss: 3.5389\n",
      "Epoch 1, Batch 529/782, Loss: 3.8693\n",
      "Epoch 1, Batch 530/782, Loss: 3.8003\n",
      "Epoch 1, Batch 531/782, Loss: 3.7803\n",
      "Epoch 1, Batch 532/782, Loss: 3.6945\n",
      "Epoch 1, Batch 533/782, Loss: 3.7949\n",
      "Epoch 1, Batch 534/782, Loss: 3.7988\n",
      "Epoch 1, Batch 535/782, Loss: 3.9622\n",
      "Epoch 1, Batch 536/782, Loss: 3.3328\n",
      "Epoch 1, Batch 537/782, Loss: 3.9255\n",
      "Epoch 1, Batch 538/782, Loss: 3.4365\n",
      "Epoch 1, Batch 539/782, Loss: 3.5803\n",
      "Epoch 1, Batch 540/782, Loss: 3.2653\n",
      "Epoch 1, Batch 541/782, Loss: 3.7900\n",
      "Epoch 1, Batch 542/782, Loss: 3.6983\n",
      "Epoch 1, Batch 543/782, Loss: 3.3673\n",
      "Epoch 1, Batch 544/782, Loss: 3.9271\n",
      "Epoch 1, Batch 545/782, Loss: 3.8759\n",
      "Epoch 1, Batch 546/782, Loss: 3.6296\n",
      "Epoch 1, Batch 547/782, Loss: 3.5112\n",
      "Epoch 1, Batch 548/782, Loss: 3.5628\n",
      "Epoch 1, Batch 549/782, Loss: 3.5424\n",
      "Epoch 1, Batch 550/782, Loss: 3.5874\n",
      "Epoch 1, Batch 551/782, Loss: 3.5453\n",
      "Epoch 1, Batch 552/782, Loss: 3.6961\n",
      "Epoch 1, Batch 553/782, Loss: 3.5028\n",
      "Epoch 1, Batch 554/782, Loss: 3.3820\n",
      "Epoch 1, Batch 555/782, Loss: 3.9314\n",
      "Epoch 1, Batch 556/782, Loss: 3.3003\n",
      "Epoch 1, Batch 557/782, Loss: 3.5786\n",
      "Epoch 1, Batch 558/782, Loss: 3.6268\n",
      "Epoch 1, Batch 559/782, Loss: 3.7584\n",
      "Epoch 1, Batch 560/782, Loss: 3.5726\n",
      "Epoch 1, Batch 561/782, Loss: 3.9932\n",
      "Epoch 1, Batch 562/782, Loss: 3.6127\n",
      "Epoch 1, Batch 563/782, Loss: 3.5589\n",
      "Epoch 1, Batch 564/782, Loss: 3.6325\n",
      "Epoch 1, Batch 565/782, Loss: 3.5130\n",
      "Epoch 1, Batch 566/782, Loss: 3.7670\n",
      "Epoch 1, Batch 567/782, Loss: 3.5616\n",
      "Epoch 1, Batch 568/782, Loss: 3.5189\n",
      "Epoch 1, Batch 569/782, Loss: 3.6195\n",
      "Epoch 1, Batch 570/782, Loss: 3.5719\n",
      "Epoch 1, Batch 571/782, Loss: 3.5204\n",
      "Epoch 1, Batch 572/782, Loss: 3.8864\n",
      "Epoch 1, Batch 573/782, Loss: 3.7294\n",
      "Epoch 1, Batch 574/782, Loss: 3.9344\n",
      "Epoch 1, Batch 575/782, Loss: 3.5916\n",
      "Epoch 1, Batch 576/782, Loss: 3.4748\n",
      "Epoch 1, Batch 577/782, Loss: 3.8305\n",
      "Epoch 1, Batch 578/782, Loss: 3.6758\n",
      "Epoch 1, Batch 579/782, Loss: 3.5091\n",
      "Epoch 1, Batch 580/782, Loss: 3.7548\n",
      "Epoch 1, Batch 581/782, Loss: 3.3919\n",
      "Epoch 1, Batch 582/782, Loss: 3.6378\n",
      "Epoch 1, Batch 583/782, Loss: 3.5501\n",
      "Epoch 1, Batch 584/782, Loss: 3.8130\n",
      "Epoch 1, Batch 585/782, Loss: 3.6834\n",
      "Epoch 1, Batch 586/782, Loss: 3.8827\n",
      "Epoch 1, Batch 587/782, Loss: 3.8359\n",
      "Epoch 1, Batch 588/782, Loss: 3.8422\n",
      "Epoch 1, Batch 589/782, Loss: 3.7918\n",
      "Epoch 1, Batch 590/782, Loss: 3.6170\n",
      "Epoch 1, Batch 591/782, Loss: 3.4358\n",
      "Epoch 1, Batch 592/782, Loss: 3.1767\n",
      "Epoch 1, Batch 593/782, Loss: 3.5304\n",
      "Epoch 1, Batch 594/782, Loss: 3.5309\n",
      "Epoch 1, Batch 595/782, Loss: 3.4292\n",
      "Epoch 1, Batch 596/782, Loss: 3.4253\n",
      "Epoch 1, Batch 597/782, Loss: 3.9131\n",
      "Epoch 1, Batch 598/782, Loss: 3.8462\n",
      "Epoch 1, Batch 599/782, Loss: 3.7903\n",
      "Epoch 1, Batch 600/782, Loss: 3.7852\n",
      "Epoch 1, Batch 601/782, Loss: 3.5661\n",
      "Epoch 1, Batch 602/782, Loss: 4.0042\n",
      "Epoch 1, Batch 603/782, Loss: 3.3221\n",
      "Epoch 1, Batch 604/782, Loss: 3.8864\n",
      "Epoch 1, Batch 605/782, Loss: 3.8047\n",
      "Epoch 1, Batch 606/782, Loss: 3.7193\n",
      "Epoch 1, Batch 607/782, Loss: 3.7267\n",
      "Epoch 1, Batch 608/782, Loss: 3.7791\n",
      "Epoch 1, Batch 609/782, Loss: 3.6937\n",
      "Epoch 1, Batch 610/782, Loss: 3.9099\n",
      "Epoch 1, Batch 611/782, Loss: 3.6797\n",
      "Epoch 1, Batch 612/782, Loss: 3.6535\n",
      "Epoch 1, Batch 613/782, Loss: 3.2885\n",
      "Epoch 1, Batch 614/782, Loss: 3.4172\n",
      "Epoch 1, Batch 615/782, Loss: 3.7355\n",
      "Epoch 1, Batch 616/782, Loss: 3.2426\n",
      "Epoch 1, Batch 617/782, Loss: 3.6814\n",
      "Epoch 1, Batch 618/782, Loss: 3.3549\n",
      "Epoch 1, Batch 619/782, Loss: 3.8532\n",
      "Epoch 1, Batch 620/782, Loss: 3.6227\n",
      "Epoch 1, Batch 621/782, Loss: 3.4320\n",
      "Epoch 1, Batch 622/782, Loss: 3.6017\n",
      "Epoch 1, Batch 623/782, Loss: 3.4442\n",
      "Epoch 1, Batch 624/782, Loss: 3.5849\n",
      "Epoch 1, Batch 625/782, Loss: 3.7731\n",
      "Epoch 1, Batch 626/782, Loss: 3.6058\n",
      "Epoch 1, Batch 627/782, Loss: 3.8339\n",
      "Epoch 1, Batch 628/782, Loss: 3.7514\n",
      "Epoch 1, Batch 629/782, Loss: 3.5932\n",
      "Epoch 1, Batch 630/782, Loss: 3.4371\n",
      "Epoch 1, Batch 631/782, Loss: 3.4845\n",
      "Epoch 1, Batch 632/782, Loss: 3.5730\n",
      "Epoch 1, Batch 633/782, Loss: 3.8248\n",
      "Epoch 1, Batch 634/782, Loss: 3.4826\n",
      "Epoch 1, Batch 635/782, Loss: 3.3178\n",
      "Epoch 1, Batch 636/782, Loss: 3.5844\n",
      "Epoch 1, Batch 637/782, Loss: 3.6904\n",
      "Epoch 1, Batch 638/782, Loss: 3.6265\n",
      "Epoch 1, Batch 639/782, Loss: 3.8051\n",
      "Epoch 1, Batch 640/782, Loss: 3.6596\n",
      "Epoch 1, Batch 641/782, Loss: 3.5731\n",
      "Epoch 1, Batch 642/782, Loss: 3.6617\n",
      "Epoch 1, Batch 643/782, Loss: 3.3746\n",
      "Epoch 1, Batch 644/782, Loss: 3.6061\n",
      "Epoch 1, Batch 645/782, Loss: 3.5204\n",
      "Epoch 1, Batch 646/782, Loss: 3.5393\n",
      "Epoch 1, Batch 647/782, Loss: 3.7436\n",
      "Epoch 1, Batch 648/782, Loss: 3.5508\n",
      "Epoch 1, Batch 649/782, Loss: 3.4840\n",
      "Epoch 1, Batch 650/782, Loss: 3.3086\n",
      "Epoch 1, Batch 651/782, Loss: 3.4085\n",
      "Epoch 1, Batch 652/782, Loss: 3.6117\n",
      "Epoch 1, Batch 653/782, Loss: 3.5809\n",
      "Epoch 1, Batch 654/782, Loss: 3.4897\n",
      "Epoch 1, Batch 655/782, Loss: 3.4463\n",
      "Epoch 1, Batch 656/782, Loss: 3.5264\n",
      "Epoch 1, Batch 657/782, Loss: 3.2938\n",
      "Epoch 1, Batch 658/782, Loss: 3.6400\n",
      "Epoch 1, Batch 659/782, Loss: 3.3307\n",
      "Epoch 1, Batch 660/782, Loss: 3.6639\n",
      "Epoch 1, Batch 661/782, Loss: 3.3801\n",
      "Epoch 1, Batch 662/782, Loss: 3.4817\n",
      "Epoch 1, Batch 663/782, Loss: 3.4812\n",
      "Epoch 1, Batch 664/782, Loss: 3.5908\n",
      "Epoch 1, Batch 665/782, Loss: 3.4088\n",
      "Epoch 1, Batch 666/782, Loss: 3.5507\n",
      "Epoch 1, Batch 667/782, Loss: 3.1950\n",
      "Epoch 1, Batch 668/782, Loss: 3.6366\n",
      "Epoch 1, Batch 669/782, Loss: 3.1833\n",
      "Epoch 1, Batch 670/782, Loss: 3.5043\n",
      "Epoch 1, Batch 671/782, Loss: 3.5982\n",
      "Epoch 1, Batch 672/782, Loss: 3.6316\n",
      "Epoch 1, Batch 673/782, Loss: 3.9485\n",
      "Epoch 1, Batch 674/782, Loss: 3.7603\n",
      "Epoch 1, Batch 675/782, Loss: 3.7028\n",
      "Epoch 1, Batch 676/782, Loss: 3.8738\n",
      "Epoch 1, Batch 677/782, Loss: 3.6542\n",
      "Epoch 1, Batch 678/782, Loss: 3.3089\n",
      "Epoch 1, Batch 679/782, Loss: 3.2667\n",
      "Epoch 1, Batch 680/782, Loss: 3.7781\n",
      "Epoch 1, Batch 681/782, Loss: 3.7398\n",
      "Epoch 1, Batch 682/782, Loss: 3.5287\n",
      "Epoch 1, Batch 683/782, Loss: 3.4130\n",
      "Epoch 1, Batch 684/782, Loss: 3.2514\n",
      "Epoch 1, Batch 685/782, Loss: 3.5950\n",
      "Epoch 1, Batch 686/782, Loss: 3.3963\n",
      "Epoch 1, Batch 687/782, Loss: 3.6086\n",
      "Epoch 1, Batch 688/782, Loss: 3.5963\n",
      "Epoch 1, Batch 689/782, Loss: 3.7706\n",
      "Epoch 1, Batch 690/782, Loss: 3.5631\n",
      "Epoch 1, Batch 691/782, Loss: 3.3949\n",
      "Epoch 1, Batch 692/782, Loss: 3.4622\n",
      "Epoch 1, Batch 693/782, Loss: 3.6692\n",
      "Epoch 1, Batch 694/782, Loss: 3.7166\n",
      "Epoch 1, Batch 695/782, Loss: 3.5252\n",
      "Epoch 1, Batch 696/782, Loss: 3.7648\n",
      "Epoch 1, Batch 697/782, Loss: 3.6658\n",
      "Epoch 1, Batch 698/782, Loss: 3.1884\n",
      "Epoch 1, Batch 699/782, Loss: 3.2654\n",
      "Epoch 1, Batch 700/782, Loss: 3.1174\n",
      "Epoch 1, Batch 701/782, Loss: 3.4829\n",
      "Epoch 1, Batch 702/782, Loss: 3.7459\n",
      "Epoch 1, Batch 703/782, Loss: 3.5613\n",
      "Epoch 1, Batch 704/782, Loss: 3.4696\n",
      "Epoch 1, Batch 705/782, Loss: 3.0828\n",
      "Epoch 1, Batch 706/782, Loss: 3.3925\n",
      "Epoch 1, Batch 707/782, Loss: 3.5861\n",
      "Epoch 1, Batch 708/782, Loss: 3.3300\n",
      "Epoch 1, Batch 709/782, Loss: 3.6537\n",
      "Epoch 1, Batch 710/782, Loss: 3.5186\n",
      "Epoch 1, Batch 711/782, Loss: 3.5889\n",
      "Epoch 1, Batch 712/782, Loss: 3.6194\n",
      "Epoch 1, Batch 713/782, Loss: 3.5307\n",
      "Epoch 1, Batch 714/782, Loss: 3.3490\n",
      "Epoch 1, Batch 715/782, Loss: 3.3790\n",
      "Epoch 1, Batch 716/782, Loss: 3.3547\n",
      "Epoch 1, Batch 717/782, Loss: 3.2227\n",
      "Epoch 1, Batch 718/782, Loss: 3.7959\n",
      "Epoch 1, Batch 719/782, Loss: 3.5680\n",
      "Epoch 1, Batch 720/782, Loss: 3.5721\n",
      "Epoch 1, Batch 721/782, Loss: 3.6318\n",
      "Epoch 1, Batch 722/782, Loss: 3.8333\n",
      "Epoch 1, Batch 723/782, Loss: 3.2940\n",
      "Epoch 1, Batch 724/782, Loss: 3.6069\n",
      "Epoch 1, Batch 725/782, Loss: 3.3906\n",
      "Epoch 1, Batch 726/782, Loss: 3.5914\n",
      "Epoch 1, Batch 727/782, Loss: 3.6990\n",
      "Epoch 1, Batch 728/782, Loss: 3.4342\n",
      "Epoch 1, Batch 729/782, Loss: 3.6793\n",
      "Epoch 1, Batch 730/782, Loss: 3.8013\n",
      "Epoch 1, Batch 731/782, Loss: 3.4691\n",
      "Epoch 1, Batch 732/782, Loss: 3.3748\n",
      "Epoch 1, Batch 733/782, Loss: 3.8108\n",
      "Epoch 1, Batch 734/782, Loss: 3.4131\n",
      "Epoch 1, Batch 735/782, Loss: 3.6278\n",
      "Epoch 1, Batch 736/782, Loss: 3.6016\n",
      "Epoch 1, Batch 737/782, Loss: 3.4867\n",
      "Epoch 1, Batch 738/782, Loss: 3.4209\n",
      "Epoch 1, Batch 739/782, Loss: 3.3132\n",
      "Epoch 1, Batch 740/782, Loss: 3.7442\n",
      "Epoch 1, Batch 741/782, Loss: 3.7545\n",
      "Epoch 1, Batch 742/782, Loss: 3.6514\n",
      "Epoch 1, Batch 743/782, Loss: 3.4994\n",
      "Epoch 1, Batch 744/782, Loss: 3.5208\n",
      "Epoch 1, Batch 745/782, Loss: 3.5330\n",
      "Epoch 1, Batch 746/782, Loss: 3.3803\n",
      "Epoch 1, Batch 747/782, Loss: 3.4365\n",
      "Epoch 1, Batch 748/782, Loss: 3.2941\n",
      "Epoch 1, Batch 749/782, Loss: 3.1435\n",
      "Epoch 1, Batch 750/782, Loss: 3.8380\n",
      "Epoch 1, Batch 751/782, Loss: 3.4190\n",
      "Epoch 1, Batch 752/782, Loss: 3.5339\n",
      "Epoch 1, Batch 753/782, Loss: 3.7609\n",
      "Epoch 1, Batch 754/782, Loss: 3.5310\n",
      "Epoch 1, Batch 755/782, Loss: 3.1884\n",
      "Epoch 1, Batch 756/782, Loss: 3.8057\n",
      "Epoch 1, Batch 757/782, Loss: 3.4093\n",
      "Epoch 1, Batch 758/782, Loss: 3.4359\n",
      "Epoch 1, Batch 759/782, Loss: 3.4715\n",
      "Epoch 1, Batch 760/782, Loss: 3.4406\n",
      "Epoch 1, Batch 761/782, Loss: 3.2609\n",
      "Epoch 1, Batch 762/782, Loss: 3.3456\n",
      "Epoch 1, Batch 763/782, Loss: 3.7147\n",
      "Epoch 1, Batch 764/782, Loss: 3.7151\n",
      "Epoch 1, Batch 765/782, Loss: 3.5334\n",
      "Epoch 1, Batch 766/782, Loss: 3.4866\n",
      "Epoch 1, Batch 767/782, Loss: 3.3737\n",
      "Epoch 1, Batch 768/782, Loss: 3.6713\n",
      "Epoch 1, Batch 769/782, Loss: 3.4877\n",
      "Epoch 1, Batch 770/782, Loss: 3.7429\n",
      "Epoch 1, Batch 771/782, Loss: 3.6581\n",
      "Epoch 1, Batch 772/782, Loss: 3.4498\n",
      "Epoch 1, Batch 773/782, Loss: 3.3622\n",
      "Epoch 1, Batch 774/782, Loss: 3.3009\n",
      "Epoch 1, Batch 775/782, Loss: 3.8596\n",
      "Epoch 1, Batch 776/782, Loss: 3.3774\n",
      "Epoch 1, Batch 777/782, Loss: 3.3683\n",
      "Epoch 1, Batch 778/782, Loss: 3.5017\n",
      "Epoch 1, Batch 779/782, Loss: 3.4962\n",
      "Epoch 1, Batch 780/782, Loss: 3.5259\n",
      "Epoch 1, Batch 781/782, Loss: 3.6002\n",
      "Epoch 1, Batch 782/782, Loss: 3.4584\n",
      "Epoch 1, Average Loss: 3.8927\n"
     ]
    }
   ],
   "source": [
    "# Datasets & loaders\n",
    "train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(1):  # adjust epochs as needed\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f8b387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:28<00:00,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy on CIFAR-100 (fine-tuned): 0.1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Top-1 Accuracy on CIFAR-100 (fine-tuned): {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a9879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the model weights (recommended)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
