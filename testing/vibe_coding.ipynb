{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d18cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Synthetic 2D classification dataset\n",
    "def generate_data(n_samples=1000):\n",
    "    X = torch.randn(n_samples, 2)\n",
    "    y = (X[:, 0] * X[:, 1] > 0).long()\n",
    "    return X, y\n",
    "\n",
    "# Simple MLP for binary classification\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Prepare data\n",
    "X, y = generate_data()\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25228ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in dataloader:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "model = SimpleNet()\n",
    "train(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80acec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.000901 seconds\n",
      "Sample logits:\n",
      "tensor([[ 6.9300, -5.3727],\n",
      "        [-0.1109,  0.0112],\n",
      "        [-4.4105,  4.7811],\n",
      "        [-0.7808,  0.7385],\n",
      "        [ 1.3083, -1.2047]])\n",
      "Sample predictions:\n",
      "tensor([0, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    outputs = model(X)\n",
    "    end = time.time()\n",
    "\n",
    "# Process outputs\n",
    "logits = outputs\n",
    "preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Baseline metrics\n",
    "print(f\"Inference time: {end - start:.6f} seconds\")\n",
    "print(f\"Sample logits:\\n{logits[:5]}\")\n",
    "print(f\"Sample predictions:\\n{preds[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08aacd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered modules:\n",
      "0 -> Linear(in_features=2, out_features=16, bias=True)\n",
      "1 -> ReLU()\n",
      "2 -> Linear(in_features=16, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Helper: get modules in forward order\n",
    "def get_forward_modules(model):\n",
    "    if isinstance(model, nn.Sequential):\n",
    "        return list(model._modules.items())\n",
    "    else:\n",
    "        raise NotImplementedError(\"This prototype assumes nn.Sequential-based models.\")\n",
    "        \n",
    "modules = get_forward_modules(model.net)\n",
    "print(\"Ordered modules:\")\n",
    "for name, layer in modules:\n",
    "    print(name, \"->\", layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c3ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.0.weight\n",
      "net.0.bias\n",
      "net.2.weight\n",
      "net.2.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# See what parameters are available\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205caa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find split point from parameter name\n",
    "def split_model_at_param(model, param_name):\n",
    "    modules = get_forward_modules(model.net)\n",
    "    \n",
    "    # Flatten layers into parameter lookup\n",
    "    param_to_module_idx = {}\n",
    "    for idx, (mod_name, layer) in enumerate(modules):\n",
    "        for pname, _ in layer.named_parameters():\n",
    "            full_name = f\"net.{mod_name}.{pname}\"\n",
    "            param_to_module_idx[full_name] = idx\n",
    "\n",
    "    # Get split index\n",
    "    split_idx = param_to_module_idx[param_name]\n",
    "\n",
    "    # Create submodels\n",
    "    layers1 = nn.Sequential(*[layer for _, layer in modules[:split_idx]])\n",
    "    layers2 = nn.Sequential(*[layer for _, layer in modules[split_idx:]])\n",
    "\n",
    "    return layers1, layers2\n",
    "\n",
    "# Example split\n",
    "param_to_split = \"net.2.weight\"\n",
    "model1, model2 = split_model_at_param(model, param_to_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca74458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_out = model(X)\n",
    "    part1_out = model1(X)\n",
    "    part2_out = model2(part1_out)\n",
    "\n",
    "# Check match\n",
    "print(torch.allclose(full_out, part2_out, atol=1e-6))  # Should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a3f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# In a Jupyter cell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from model_splitter import ModelSplitter\n",
    "\n",
    "# Define toy model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Generate toy data\n",
    "def generate_data(n=1000):\n",
    "    X = torch.randn(n, 2)\n",
    "    y = (X[:, 0] * X[:, 1] > 0).long()\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data()\n",
    "model = SimpleNet()\n",
    "\n",
    "# Quick train loop\n",
    "def train(model, X, y, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for _ in range(epochs):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train(model, X, y)\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = ModelSplitter(model, X)\n",
    "\n",
    "# Split from a layer (e.g., after first Linear)\n",
    "param_name = \"net.2.weight\"  # start of second Linear layer\n",
    "part1_out, model2 = splitter.split_from_param(param_name)\n",
    "\n",
    "# Optional: demonstrate new usage\n",
    "with torch.no_grad():\n",
    "    out_new = model2(part1_out)\n",
    "    print(torch.allclose(out_new, splitter.baseline_output, atol=1e-6))  # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92b42d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model inference time: 0.011029 sec\n",
      "Model2 (second half) inference time: 0.007837 sec\n",
      "Output identical to baseline: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from model_splitter import ModelSplitter\n",
    "\n",
    "# Define a deeper model\n",
    "class LargeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Generate input\n",
    "X_large = torch.randn(10_000, 2)  # Large batch\n",
    "\n",
    "# Initialize and warm up model\n",
    "model_large = LargeNet()\n",
    "model_large.eval()\n",
    "\n",
    "# Warm-up pass\n",
    "with torch.no_grad():\n",
    "    _ = model_large(X_large)\n",
    "\n",
    "# Baseline full inference time\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    baseline_output = model_large(X_large)\n",
    "    end = time.time()\n",
    "\n",
    "print(f\"Full model inference time: {end - start:.6f} sec\")\n",
    "\n",
    "# Split and time only second half\n",
    "splitter = ModelSplitter(model_large, X_large)\n",
    "part1_out, model2 = splitter.split_from_param(\"net.4.weight\")  # Midpoint layer\n",
    "\n",
    "# Time only model2 inference\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    output_partial = model2(part1_out)\n",
    "    end = time.time()\n",
    "\n",
    "print(f\"Model2 (second half) inference time: {end - start:.6f} sec\")\n",
    "print(\"Output identical to baseline:\", torch.allclose(output_partial, baseline_output, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f484a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to C:\\Users\\wwden/.cache\\torch\\hub\\checkpoints\\vit_b_16-c867db91.pth\n",
      "  9%|â–Š         | 28.8M/330M [02:05<21:56, 240kB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m images, labels = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 2. Load pretrained ViT and adapt to MNIST\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m vit = \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvit_b_16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m vit.heads = nn.Linear(vit.heads.in_features, \u001b[32m10\u001b[39m)  \u001b[38;5;66;03m# Replace classifier head\u001b[39;00m\n\u001b[32m     23\u001b[39m vit.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[39m, in \u001b[36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m     warnings.warn(\n\u001b[32m    136\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs.keys()),\u001b[38;5;250m \u001b[39mseparate_last=\u001b[33m'\u001b[39m\u001b[33mand \u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as positional \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m     kwargs.update(keyword_only_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[39m, in \u001b[36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[32m    226\u001b[39m     kwargs[weights_param] = default_weights_arg\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torchvision\\models\\vision_transformer.py:641\u001b[39m, in \u001b[36mvit_b_16\u001b[39m\u001b[34m(weights, progress, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[33;03mConstructs a vit_b_16 architecture from\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[33;03m`An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    637\u001b[39m \u001b[33;03m    :members:\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    639\u001b[39m weights = ViT_B_16_Weights.verify(weights)\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3072\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torchvision\\models\\vision_transformer.py:335\u001b[39m, in \u001b[36m_vision_transformer\u001b[39m\u001b[34m(patch_size, num_layers, num_heads, hidden_dim, mlp_dim, weights, progress, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m model = VisionTransformer(\n\u001b[32m    325\u001b[39m     image_size=image_size,\n\u001b[32m    326\u001b[39m     patch_size=patch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m     **kwargs,\n\u001b[32m    332\u001b[39m )\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     model.load_state_dict(\u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torchvision\\models\\_api.py:90\u001b[39m, in \u001b[36mWeightsEnum.get_state_dict\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torch\\hub.py:766\u001b[39m, in \u001b[36mload_state_dict_from_url\u001b[39m\u001b[34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[39m\n\u001b[32m    764\u001b[39m         r = HASH_REGEX.search(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[32m    765\u001b[39m         hash_prefix = r.group(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torch\\hub.py:651\u001b[39m, in \u001b[36mdownload_url_to_file\u001b[39m\u001b[34m(url, dst, hash_prefix, progress)\u001b[39m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=file_size, disable=\u001b[38;5;129;01mnot\u001b[39;00m progress,\n\u001b[32m    649\u001b[39m           unit=\u001b[33m'\u001b[39m\u001b[33mB\u001b[39m\u001b[33m'\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor=\u001b[32m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m         buffer = \u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) == \u001b[32m0\u001b[39m:\n\u001b[32m    653\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:473\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    472\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m s = \u001b[38;5;28mself\u001b[39m.fp.read(amt)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from model_splitter import ModelSplitter\n",
    "import time\n",
    "\n",
    "# --- 1. Minimal ViT block ---\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=7, emb_dim=64, depth=2, n_heads=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = 1 * patch_size * patch_size  # for grayscale input\n",
    "\n",
    "        self.patch_embed = nn.Linear(self.patch_dim, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 1, emb_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        p = int((H * W) // self.n_patches)**0.5\n",
    "        x = x.unfold(2, int(p), int(p)).unfold(3, int(p), int(p))  # (B, C, nH, nW, p, p)\n",
    "        x = x.contiguous().view(B, C, -1, int(p), int(p)).permute(0, 2, 1, 3, 4)\n",
    "        x = x.reshape(B, self.n_patches, -1)  # (B, n_patches, patch_dim)\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x += self.pos_embed\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        cls_out = x[:, 0]\n",
    "        return self.mlp_head(cls_out)\n",
    "\n",
    "# --- 2. Load MNIST ---\n",
    "transform = Compose([ToTensor()])\n",
    "mnist = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "loader = DataLoader(mnist, batch_size=64, shuffle=False)\n",
    "images, labels = next(iter(loader))\n",
    "\n",
    "# --- 3. Create and run model ---\n",
    "model = TinyViT()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    full_out = model(images)\n",
    "    end = time.time()\n",
    "print(f\"Full TinyViT inference time: {end - start:.6f} sec\")\n",
    "\n",
    "# --- 4. Use ModelSplitter ---\n",
    "splitter = ModelSplitter(model, images)\n",
    "\n",
    "# Check available parameter names\n",
    "# for name, _ in model.named_parameters(): print(name)\n",
    "\n",
    "part1_out, model2 = splitter.split_from_param(\"mlp_head.1.weight\")\n",
    "\n",
    "# --- 5. Partial inference ---\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    part_out = model2(part1_out)\n",
    "    end = time.time()\n",
    "print(f\"TinyViT head-only inference time: {end - start:.6f} sec\")\n",
    "print(\"Output matches:\", torch.allclose(part_out, full_out, atol=1e-6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
