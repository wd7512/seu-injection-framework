{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d18cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Synthetic 2D classification dataset\n",
    "def generate_data(n_samples=1000):\n",
    "    X = torch.randn(n_samples, 2)\n",
    "    y = (X[:, 0] * X[:, 1] > 0).long()\n",
    "    return X, y\n",
    "\n",
    "# Simple MLP for binary classification\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Prepare data\n",
    "X, y = generate_data()\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25228ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in dataloader:\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "model = SimpleNet()\n",
    "train(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80acec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.000901 seconds\n",
      "Sample logits:\n",
      "tensor([[ 6.9300, -5.3727],\n",
      "        [-0.1109,  0.0112],\n",
      "        [-4.4105,  4.7811],\n",
      "        [-0.7808,  0.7385],\n",
      "        [ 1.3083, -1.2047]])\n",
      "Sample predictions:\n",
      "tensor([0, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    outputs = model(X)\n",
    "    end = time.time()\n",
    "\n",
    "# Process outputs\n",
    "logits = outputs\n",
    "preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Baseline metrics\n",
    "print(f\"Inference time: {end - start:.6f} seconds\")\n",
    "print(f\"Sample logits:\\n{logits[:5]}\")\n",
    "print(f\"Sample predictions:\\n{preds[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08aacd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered modules:\n",
      "0 -> Linear(in_features=2, out_features=16, bias=True)\n",
      "1 -> ReLU()\n",
      "2 -> Linear(in_features=16, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Helper: get modules in forward order\n",
    "def get_forward_modules(model):\n",
    "    if isinstance(model, nn.Sequential):\n",
    "        return list(model._modules.items())\n",
    "    else:\n",
    "        raise NotImplementedError(\"This prototype assumes nn.Sequential-based models.\")\n",
    "        \n",
    "modules = get_forward_modules(model.net)\n",
    "print(\"Ordered modules:\")\n",
    "for name, layer in modules:\n",
    "    print(name, \"->\", layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c3ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.0.weight\n",
      "net.0.bias\n",
      "net.2.weight\n",
      "net.2.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# See what parameters are available\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205caa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find split point from parameter name\n",
    "def split_model_at_param(model, param_name):\n",
    "    modules = get_forward_modules(model.net)\n",
    "    \n",
    "    # Flatten layers into parameter lookup\n",
    "    param_to_module_idx = {}\n",
    "    for idx, (mod_name, layer) in enumerate(modules):\n",
    "        for pname, _ in layer.named_parameters():\n",
    "            full_name = f\"net.{mod_name}.{pname}\"\n",
    "            param_to_module_idx[full_name] = idx\n",
    "\n",
    "    # Get split index\n",
    "    split_idx = param_to_module_idx[param_name]\n",
    "\n",
    "    # Create submodels\n",
    "    layers1 = nn.Sequential(*[layer for _, layer in modules[:split_idx]])\n",
    "    layers2 = nn.Sequential(*[layer for _, layer in modules[split_idx:]])\n",
    "\n",
    "    return layers1, layers2\n",
    "\n",
    "# Example split\n",
    "param_to_split = \"net.2.weight\"\n",
    "model1, model2 = split_model_at_param(model, param_to_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca74458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_out = model(X)\n",
    "    part1_out = model1(X)\n",
    "    part2_out = model2(part1_out)\n",
    "\n",
    "# Check match\n",
    "print(torch.allclose(full_out, part2_out, atol=1e-6))  # Should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a3f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# In a Jupyter cell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from model_splitter import ModelSplitter\n",
    "\n",
    "# Define toy model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Generate toy data\n",
    "def generate_data(n=1000):\n",
    "    X = torch.randn(n, 2)\n",
    "    y = (X[:, 0] * X[:, 1] > 0).long()\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data()\n",
    "model = SimpleNet()\n",
    "\n",
    "# Quick train loop\n",
    "def train(model, X, y, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for _ in range(epochs):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train(model, X, y)\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = ModelSplitter(model, X)\n",
    "\n",
    "# Split from a layer (e.g., after first Linear)\n",
    "param_name = \"net.2.weight\"  # start of second Linear layer\n",
    "part1_out, model2 = splitter.split_from_param(param_name)\n",
    "\n",
    "# Optional: demonstrate new usage\n",
    "with torch.no_grad():\n",
    "    out_new = model2(part1_out)\n",
    "    print(torch.allclose(out_new, splitter.baseline_output, atol=1e-6))  # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a92b42d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model inference time: 0.011955 sec\n",
      "Model2 (second half) inference time: 0.007741 sec\n",
      "Output identical to baseline: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from model_splitter import ModelSplitter\n",
    "\n",
    "# Define a deeper model\n",
    "class LargeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Generate input\n",
    "X_large = torch.randn(10_000, 2)  # Large batch\n",
    "\n",
    "# Initialize and warm up model\n",
    "model_large = LargeNet()\n",
    "model_large.eval()\n",
    "\n",
    "# Warm-up pass\n",
    "with torch.no_grad():\n",
    "    _ = model_large(X_large)\n",
    "\n",
    "# Baseline full inference time\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    baseline_output = model_large(X_large)\n",
    "    end = time.time()\n",
    "\n",
    "print(f\"Full model inference time: {end - start:.6f} sec\")\n",
    "\n",
    "# Split and time only second half\n",
    "splitter = ModelSplitter(model_large, X_large)\n",
    "part1_out, model2 = splitter.split_from_param(\"net.4.weight\")  # Midpoint layer\n",
    "\n",
    "# Time only model2 inference\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    output_partial = model2(part1_out)\n",
    "    end = time.time()\n",
    "\n",
    "print(f\"Model2 (second half) inference time: {end - start:.6f} sec\")\n",
    "print(\"Output identical to baseline:\", torch.allclose(output_partial, baseline_output, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f484a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full TinyViT inference time: 0.025628 sec\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1792x28 and 49x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     62\u001b[39m splitter = ModelSplitter(model, images)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Check available parameter names\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# for name, _ in model.named_parameters(): print(name)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m part1_out, model2 = \u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_from_param\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmlp_head.1.weight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# --- 5. Partial inference ---\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\testing\\model_splitter.py:86\u001b[39m, in \u001b[36mModelSplitter.split_from_param\u001b[39m\u001b[34m(self, param_name)\u001b[39m\n\u001b[32m     83\u001b[39m model2 = Part2(ops2)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     part1_out = \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     recombined_out = model2(part1_out)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.allclose(recombined_out, \u001b[38;5;28mself\u001b[39m.baseline_output, atol=\u001b[32m1e-6\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\testing\\model_splitter.py:66\u001b[39m, in \u001b[36mModelSplitter.split_from_param.<locals>.Part1.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m mod, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ops:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m         x = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositories\\seu-injection-framework\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1792x28 and 49x64)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from model_splitter import ModelSplitter\n",
    "import time\n",
    "\n",
    "# --- 1. Minimal ViT block ---\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=7, emb_dim=64, depth=2, n_heads=4, n_classes=10):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = 1 * patch_size * patch_size  # for grayscale input\n",
    "\n",
    "        self.patch_embed = nn.Linear(self.patch_dim, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 1, emb_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        p = int((H * W) // self.n_patches)**0.5\n",
    "        x = x.unfold(2, int(p), int(p)).unfold(3, int(p), int(p))  # (B, C, nH, nW, p, p)\n",
    "        x = x.contiguous().view(B, C, -1, int(p), int(p)).permute(0, 2, 1, 3, 4)\n",
    "        x = x.reshape(B, self.n_patches, -1)  # (B, n_patches, patch_dim)\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x += self.pos_embed\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        cls_out = x[:, 0]\n",
    "        return self.mlp_head(cls_out)\n",
    "\n",
    "# --- 2. Load MNIST ---\n",
    "transform = Compose([ToTensor()])\n",
    "mnist = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "loader = DataLoader(mnist, batch_size=64, shuffle=False)\n",
    "images, labels = next(iter(loader))\n",
    "\n",
    "# --- 3. Create and run model ---\n",
    "model = TinyViT()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    full_out = model(images)\n",
    "    end = time.time()\n",
    "print(f\"Full TinyViT inference time: {end - start:.6f} sec\")\n",
    "\n",
    "# --- 4. Use ModelSplitter ---\n",
    "splitter = ModelSplitter(model, images)\n",
    "\n",
    "# Check available parameter names\n",
    "# for name, _ in model.named_parameters(): print(name)\n",
    "\n",
    "part1_out, model2 = splitter.split_from_param(\"mlp_head.1.weight\")\n",
    "\n",
    "# --- 5. Partial inference ---\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    part_out = model2(part1_out)\n",
    "    end = time.time()\n",
    "print(f\"TinyViT head-only inference time: {end - start:.6f} sec\")\n",
    "print(\"Output matches:\", torch.allclose(part_out, full_out, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623c3c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing SimpleNN:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelSplitter' object has no attribute 'legal_splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     81\u001b[39m vit_in = torch.randn(\u001b[32m5\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# --- Run tests ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSimpleNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSimpleNN\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m test_model(SimpleCNN(), cnn_in, \u001b[33m\"\u001b[39m\u001b[33mSimpleCNN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m test_model(SimpleRNN(), rnn_in, \u001b[33m\"\u001b[39m\u001b[33mSimpleRNN\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(model, sample_input, name)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTesting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m splitter = ModelSplitter(model, sample_input)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLegal split points:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlegal_splits\u001b[49m))\n\u001b[32m     69\u001b[39m safe_points = splitter.get_safe_split_points()\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSafe split points:\u001b[39m\u001b[33m\"\u001b[39m, safe_points)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ModelSplitter' object has no attribute 'legal_splits'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_splitter import ModelSplitter\n",
    "\n",
    "# --- Models ---\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(4, 5)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(10, 20, batch_first=True)\n",
    "        self.fc = nn.Linear(20, 5)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1])\n",
    "\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # minimal ViT like before (simplified)\n",
    "        self.patch_embed = nn.Linear(16, 32)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,32))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 5, 32))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(32), nn.Linear(32, 3))\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, 4, 4)  # pretend 4 patches each of dim 4\n",
    "        x = self.patch_embed(x)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer(x)\n",
    "        return self.mlp_head(x[:, 0])\n",
    "\n",
    "\n",
    "# --- Test function ---\n",
    "\n",
    "def test_model(model, sample_input, name):\n",
    "    print(f\"\\nTesting {name}:\")\n",
    "    splitter = ModelSplitter(model, sample_input)\n",
    "    print(\"Legal split points:\", sorted(splitter.legal_splits))\n",
    "    safe_points = splitter.get_safe_split_points()\n",
    "    print(\"Safe split points:\", safe_points)\n",
    "    for sp in safe_points:\n",
    "        print(f\"  Splitting at {sp} ...\", end=\"\")\n",
    "        part1_out, model2 = splitter.split_from_param(sp)\n",
    "        print(\" Success.\")\n",
    "\n",
    "# --- Sample inputs ---\n",
    "\n",
    "nn_in = torch.randn(5, 20)\n",
    "cnn_in = torch.randn(5, 1, 8, 8)\n",
    "rnn_in = torch.randn(5, 7, 10)\n",
    "vit_in = torch.randn(5, 1, 4, 4)\n",
    "\n",
    "# --- Run tests ---\n",
    "\n",
    "test_model(SimpleNN(), nn_in, \"SimpleNN\")\n",
    "test_model(SimpleCNN(), cnn_in, \"SimpleCNN\")\n",
    "test_model(SimpleRNN(), rnn_in, \"SimpleRNN\")\n",
    "test_model(TinyViT(), vit_in, \"TinyViT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05efc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
