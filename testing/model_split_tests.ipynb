{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6732d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing SimpleNN:\n",
      "Legal split points: ['net.0.bias', 'net.0.weight', 'net.2.bias', 'net.2.weight']\n",
      "  Trying to split at net.0.bias ... Success.\n",
      "  Trying to split at net.0.weight ... Success.\n",
      "  Trying to split at net.2.bias ... Success.\n",
      "  Trying to split at net.2.weight ... Success.\n",
      "\n",
      "Testing SimpleCNN:\n",
      "Legal split points: ['conv.0.bias', 'conv.0.weight', 'conv.2.bias', 'conv.2.weight', 'fc.bias', 'fc.weight']\n",
      "  Trying to split at conv.0.bias ... Failed. Error: mat1 and mat2 shapes cannot be multiplied (20x1 and 4x5)\n",
      "  Trying to split at conv.0.weight ... Failed. Error: mat1 and mat2 shapes cannot be multiplied (20x1 and 4x5)\n",
      "  Trying to split at conv.2.bias ... Failed. Error: mat1 and mat2 shapes cannot be multiplied (20x1 and 4x5)\n",
      "  Trying to split at conv.2.weight ... Failed. Error: mat1 and mat2 shapes cannot be multiplied (20x1 and 4x5)\n",
      "  Trying to split at fc.bias ... Failed. Error: mat1 and mat2 shapes cannot be multiplied (20x1 and 4x5)\n",
      "  Trying to split at fc.weight ... Failed. Error: mat1 and mat2 shapes cannot be multiplied (20x1 and 4x5)\n",
      "\n",
      "Testing SimpleRNN:\n",
      "Legal split points: ['fc.bias', 'fc.weight', 'lstm.bias_hh_l0', 'lstm.bias_ih_l0', 'lstm.weight_hh_l0', 'lstm.weight_ih_l0']\n",
      "  Trying to split at fc.bias ... Failed. Error: linear(): argument 'input' (position 1) must be Tensor, not tuple\n",
      "  Trying to split at fc.weight ... Failed. Error: linear(): argument 'input' (position 1) must be Tensor, not tuple\n",
      "  Trying to split at lstm.bias_hh_l0 ... Failed. Error: linear(): argument 'input' (position 1) must be Tensor, not tuple\n",
      "  Trying to split at lstm.bias_ih_l0 ... Failed. Error: linear(): argument 'input' (position 1) must be Tensor, not tuple\n",
      "  Trying to split at lstm.weight_hh_l0 ... Failed. Error: linear(): argument 'input' (position 1) must be Tensor, not tuple\n",
      "  Trying to split at lstm.weight_ih_l0 ... Failed. Error: linear(): argument 'input' (position 1) must be Tensor, not tuple\n",
      "\n",
      "Testing TinyViT:\n",
      "Legal split points: ['mlp_head.0.bias', 'mlp_head.0.weight', 'mlp_head.1.bias', 'mlp_head.1.weight', 'patch_embed.bias', 'patch_embed.weight']\n",
      "  Trying to split at mlp_head.0.bias ... Failed. Error: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2\n",
      "  Trying to split at mlp_head.0.weight ... Failed. Error: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2\n",
      "  Trying to split at mlp_head.1.bias ... Failed. Error: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2\n",
      "  Trying to split at mlp_head.1.weight ... Failed. Error: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2\n",
      "  Trying to split at patch_embed.bias ... Failed. Error: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2\n",
      "  Trying to split at patch_embed.weight ... Failed. Error: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_splitter import ModelSplitter\n",
    "\n",
    "# --- Models ---\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(4, 5)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(10, 20, batch_first=True)\n",
    "        self.fc = nn.Linear(20, 5)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1])\n",
    "\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Linear(4, 32)  # corrected input dim to 4\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,32))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 5, 32))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(32), nn.Linear(32, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, 4, 4)  # 4 patches, each patch dim 4\n",
    "        x = self.patch_embed(x)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer(x)\n",
    "        return self.mlp_head(x[:, 0])\n",
    "\n",
    "# --- Test function ---\n",
    "\n",
    "# --- Sample inputs ---\n",
    "\n",
    "nn_in = torch.randn(5, 20)\n",
    "cnn_in = torch.randn(5, 1, 8, 8)\n",
    "rnn_in = torch.randn(5, 7, 10)\n",
    "vit_in = torch.randn(5, 1, 4, 4)\n",
    "\n",
    "def test_model(model, sample_input, name):\n",
    "    print(f\"\\nTesting {name}:\")\n",
    "    splitter = ModelSplitter(model, sample_input)\n",
    "    print(\"Legal split points:\", sorted(splitter.legal_splits))\n",
    "    # We test on all legal splits, regardless of safe check\n",
    "    for param_name in sorted(splitter.legal_splits):\n",
    "        print(f\"  Trying to split at {param_name} ...\", end=\"\")\n",
    "        try:\n",
    "            part1_out, model2 = splitter.split_from_param(param_name)\n",
    "            print(\" Success.\")\n",
    "        except Exception as e:\n",
    "            print(f\" Failed. Error: {e}\")\n",
    "\n",
    "# Then run as before:\n",
    "test_model(SimpleNN(), nn_in, \"SimpleNN\")\n",
    "test_model(SimpleCNN(), cnn_in, \"SimpleCNN\")\n",
    "test_model(SimpleRNN(), rnn_in, \"SimpleRNN\")\n",
    "test_model(TinyViT(), vit_in, \"TinyViT\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
